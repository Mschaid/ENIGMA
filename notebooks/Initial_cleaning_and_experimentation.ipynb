{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "\n",
    "#custom imports\n",
    "from src.processors.Preprocessor import Preprocessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = r'/Users/michaelschaid/GitHub/dopamine_modeling/data/gaby_test/aggregated_data.parquet.gzp'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial preprocessing of data:\n",
    "* load data\n",
    "* hot encode categorical variables\n",
    "* select features and target variable\n",
    "* split data into train and test sets\n",
    "  * started withn 80/20 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor  = Preprocessor(PATH)\n",
    "preprocessor.load_data()\n",
    "preprocessor.one_hot_encode(*['event', 'sensor'])\n",
    "preprocessor.dummy_data\n",
    "\n",
    "X_labels = ['day', 'time', 'trial', 'signal','event_cue',  'event_shock', 'sensor_D1', 'sensor_D2',\n",
    "       'sensor_DA']\n",
    "y_label = 'event_avoid'\n",
    "\n",
    "preprocessor.split_train_test(X_labels = X_labels, y_label=y_label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial model build\n",
    "* starting with a Time Series Classification model to predict avoid\n",
    "  * Conv1D\n",
    "  * MaxPool1\n",
    "  * Flatten\n",
    "  * dense (softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = int(preprocessor.dummy_data.time.unique().shape[0]/45) #1\n",
    "num_features = preprocessor.X_train.shape[1]\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    Conv1D(filters = 64, kernel_size = 3, activation = 'relu', input_shape = (window_size, num_features)), \n",
    "    MaxPooling1D(pool_size = 2),\n",
    "    Flatten(),\n",
    "    Dense(10, activation = 'softmax')\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#todo need to reshape data for fitting\n",
    "model.compile(optimizer = 'adam', \n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics = ['accuracy'])\n",
    "model.fit(preprocessor.X_train, \n",
    "          preprocessor.y_train, \n",
    "          epochs = 100, \n",
    "          validation_data = (preprocessor.X_test, preprocessor.y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window_data = tf.keras.preprocessing.timeseries_dataset_from_array(preprocessor.dummy_data, \n",
    "                                                                        targets = )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_sample = preprocessor.dummy_data.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_sample.melt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dope",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
