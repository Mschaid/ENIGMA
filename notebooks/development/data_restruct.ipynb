{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec # for subplots\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import seaborn.objects as so\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from src.utilities.pandas_helpers import get_features\n",
    "\n",
    "\n",
    "plt.rcParams['path.simplify_threshold'] = 1.0\n",
    "plt.rcParams['agg.path.chunksize'] = 2000 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives to restructer data:\n",
    "* one hot encode mice (done)\n",
    "* encode cyclic time : time_sin and time_cos\n",
    "* Dim reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_PATH = '/projects/p31961/gaby_data/aggregated_data/data_pipeline_full_dataset/datasets/full_dataset.parquet.gzip'\n",
    "DATA_PATH = \"/Users/michaelschaid/interm_data_transfer/full_dataset.parquet.gzip\" #local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet(DATA_PATH)\n",
    "# data = data[::1000].reset_index(drop=True) # subsample for speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Encoding cyclic time\n",
    "def cyclic_time(df):\n",
    "    max_time = df.time.max()\n",
    "    return (\n",
    "        df\n",
    "        .assign(\n",
    "            time_max_norm = lambda df_: (df_.time / max_time) * (2 * np.pi),\n",
    "            time_cos = lambda df_: np.cos(df_.time_max_norm),\n",
    "            time_sin = lambda df_: np.sin(df_.time_max_norm)\n",
    "             \n",
    "            \n",
    "                )\n",
    "        .sort_values(by=['time', 'trial_count'])\n",
    "        .reset_index(drop=True)\n",
    "        )\n",
    "update_df = cyclic_time(data)\n",
    "sig = update_df.query(\"mouse_id_7==1 & sensor_DA == 1\").sort_values(by=['time', 'trial_count'])\n",
    "sig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(40, 40))\n",
    "plt.matshow(np.corrcoef(data.corr()), cmap = 'plasma')\n",
    "# Set the tick labels for the x and y axes\n",
    "plt.xticks(range(len(data.columns)), data.columns, rotation=90)\n",
    "plt.yticks(range(len(data.columns)), data.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concept for data structure \n",
    "<span style=\"color:red\">**batch size**</span> since we are training for trial_count, we consider each subject (mouse) a batch. \n",
    "\n",
    "<span style=\"color:red\">**Sequence length**</span> This is the number of time steps, in this case we consider 1 time step, 1 sequence length. Since the trials are different lengths, we will need to pad the shorter sequences. We will use a value of 1000 filled with zeros, so our model will learn to ignore them. *we might be able to get away with a lambda layer* \n",
    "\n",
    "<span style=\"color:red\">**Number of features**</span> everything else, including time (seconds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing for LSTM input \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semgementation \n",
    "*  Spit data into seperate dataframes for each subject\n",
    "*  Also need to restructure so signal for each event is seperated, and we will predict all 4 signals from the given data. Might have issues with missing data, but hopefully padding corrects that\n",
    "\n",
    "## Drop Unnessary columns\n",
    "\n",
    "* Since we are segmenting by subject, we can drop mouse_id columns\n",
    "* we need to track this data externally, we can do so in pandas and save it\n",
    "  \n",
    "## Pad sequences \n",
    "* to control for varying length \n",
    "``` {python}\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequence\n",
    "```\n",
    "\n",
    "## Reshape data for LSTM\n",
    "``` LSTM(num_seq, sequence_length, num_features)```\n",
    "\n",
    "* num_seq: number of subjects\n",
    "\n",
    "* sequence_length: number of trials\n",
    "\n",
    "* num_features: features minus trial_count and subject_identifies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate events into seperate signal colums: cue, shock, escape, avoid\n",
    "def seperate_events(df):\n",
    "    return (df\n",
    "            .assign(cue = lambda df_: df_.query(\"event_cue==1\").signal, \n",
    "                    shock = lambda df_: df_.query(\"event_shock==1\").signal,\n",
    "                    escape = lambda df_: df_.query(\"event_escape==1\").signal,\n",
    "                    avoid = lambda df_: df_.query(\"event_avoid==1\").signal)\n",
    "            .query(\"sensor_DA == 1\")\n",
    "            .drop(columns=['signal', 'event_cue', 'event_shock', 'event_escape', 'event_avoid', 'sensor_DA', 'sensor_D1', 'sensor_D2', 'learning_phase', 'latency'])\n",
    "            .dropna(subset=['cue'])\n",
    "    )\n",
    "seperated_signal = seperate_events(data)\n",
    "seperated_signal.sort_values(by=['time', 'trial_count']).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seperated_signal.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subject_columns(df, prefix):\n",
    "    return [col for col in df.columns if col.startswith(prefix)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort_values(by = ['trial_count', 'time']).reset_index(drop=True).query(\"mouse_id_7==1 & trial ==1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enigma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
